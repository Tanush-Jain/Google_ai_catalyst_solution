<parameter name="content"># SentinelLLM Project Summary

## ğŸ¯ Project Overview

**SentinelLLM** is a production-grade observability and incident response platform for Large Language Model (LLM) applications running on Google Cloud. It's built for the **AI Partner Catalyst Challenge (Datadog Track)**.

### Core Purpose
Transform LLM behavior into measurable telemetry and actionable incidents using Datadog, making LLM failures (hallucinations, prompt injection, latency regressions, cost explosions, data leakage) visible, measurable, and actionable.

### Architecture
```
Client â†’ SentinelLLM Gateway â†’ Vertex AI / Gemini
                      â†“
               OpenTelemetry
                      â†“
                  Datadog
```

## ğŸ“ Current Project Structure

```
Google_ai_catalyst_solution/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker/
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ infra/
â”‚   â””â”€â”€ datadog/
â”‚       â”œâ”€â”€ dashboards/
â”‚       â”‚   â””â”€â”€ sentinel_llm_dashboard.json
â”‚       â””â”€â”€ monitors/
â”‚           â”œâ”€â”€ cost_spike_monitor.json
â”‚           â”œâ”€â”€ error_rate_monitor.json
â”‚           â”œâ”€â”€ latency_monitor.json
â”‚           â””â”€â”€ prompt_injection_monitor.json
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ simulate_attack.py
â”‚   â””â”€â”€ simulate_cost_spike.py
â””â”€â”€ src/
    â”œâ”€â”€ __init__.py (âœ… FIXED - Added)
    â”œâ”€â”€ main.py
    â”œâ”€â”€ api/
    â”‚   â”œâ”€â”€ __init__.py (âœ… FIXED - Added)
    â”‚   â””â”€â”€ routes.py
    â”œâ”€â”€ gateway/
    â”‚   â”œâ”€â”€ __init__.py (âœ… FIXED - Added)
    â”‚   â”œâ”€â”€ app.py
    â”‚   â”œâ”€â”€ config.py
    â”‚   â”œâ”€â”€ llm_client.py
    â”‚   â”œâ”€â”€ security.py
    â”‚   â””â”€â”€ telemetry.py
    â””â”€â”€ utils/
        â”œâ”€â”€ __init__.py (âœ… FIXED - Added)
        â””â”€â”€ token_counter.py
```

## ğŸ”§ Tech Stack

### Google Cloud
- Vertex AI (Gemini)
- Cloud Run / GKE
- OpenTelemetry

### Observability
- Datadog APM
- Datadog Logs
- Datadog Metrics
- Datadog Incident Management

### Backend
- Python (FastAPI)
- Docker

## ğŸš¨ Key Observability Signals

### Performance
- LLM latency (p50 / p95)
- Error rate
- Retry count

### Cost
- Input / output token count
- Estimated cost per request
- Cost per minute

### Quality
- Response confidence score
- Hallucination risk score

### Security
- Prompt injection detection
- PII exposure detection
- Jailbreak attempts

## ğŸ› ï¸ Current Status & Issues Fixed

### âœ… PROBLEM IDENTIFIED
**Issue**: `ModuleNotFoundError: No module named 'src'` when running:
```bash
uvicorn src.main:app --host 0.0.0.0 --port 8080
```

**Root Cause**: Missing `__init__.py` files in Python packages

### âœ… SOLUTION IMPLEMENTED

**Fixed Python Package Structure** by adding missing `__init__.py` files:

1. **src/__init__.py** - Package metadata and exports
2. **src/api/__init__.py** - API package exports  
3. **src/gateway/__init__.py** - Gateway package exports
4. **src/utils/__init__.py** - Utils package exports

### âœ… APPLICATION ENTRYPOINT
The `src/main.py` is properly configured as production entry point that:
- Imports FastAPI app from `src.api.routes`
- Configures middleware, telemetry, and startup/shutdown handlers
- Uses uvicorn with correct host/port configuration

## ğŸš€ Correct Run Commands

**From project root** (`/Users/wopseeion/Programming/Hackathons/Googe_catalyst/Google_ai_catalyst_solution`):

```bash
# Method 1: Using uvicorn directly
uvicorn src.main:app --host 0.0.0.0 --port 8080 --reload

# Method 2: Using the main module
python src/main.py

# Method 3: With virtual environment
source ../venv/bin/activate
uvicorn src.main:app --host 0.0.0.0 --port 8080 --reload
```

## ğŸ“Š Key Files Analysis

### src/main.py
- Production FastAPI application entry point
- Configures middleware (CORS, TrustedHost)
- Sets up startup/shutdown event handlers
- Initializes telemetry and OpenTelemetry instrumentation

### src/api/routes.py
- FastAPI routes and endpoints
- LLM interaction endpoints
- Health check endpoints
- Request/response handling

### src/gateway/
- **config.py**: Configuration management
- **telemetry.py**: OpenTelemetry instrumentation
- **llm_client.py**: Vertex AI/Gemini integration
- **security.py**: Security checks and validation

### src/utils/
- **token_counter.py**: Token counting and estimation

## ğŸŒ Environment Requirements

Required environment variables:
```env
GCP_PROJECT_ID=your_project_id
VERTEX_LOCATION=us-central1
DATADOG_API_KEY=your_datadog_api_key
DATADOG_SITE=datadoghq.com
GEMINI_MODEL=gemini-1.5-pro
```

## ğŸ¯ Automated Incident Detection

SentinelLLM defines Datadog detection rules for:
- Prompt Injection Attempts
- Cost Explosions
- Latency Regressions
- Hallucination Risk Spikes
- Sensitive Data Leakage

## ğŸ“¦ Dependencies (requirements.txt)

Key dependencies include:
- FastAPI
- uvicorn
- google-cloud-aiplatform
- opentelemetry-api
- opentelemetry-sdk
- ddtrace (Datadog)
- python-dotenv
- pydantic

## ğŸš€ Next Steps / TODO

1. **Test Application**: Verify the fixed imports work correctly
2. **Environment Setup**: Configure required environment variables
3. **Datadog Integration**: Set up Datadog dashboards and monitors
4. **Google Cloud Deployment**: Deploy to Cloud Run or GKE
5. **Testing**: Run simulation scripts for attack/cost spike detection

## ğŸ¥ Demo Capabilities

The project includes simulation scripts:
- `scripts/simulate_attack.py` - Simulates prompt injection attacks
- `scripts/simulate_cost_spike.py` - Simulates cost explosions

These trigger Datadog detection rules and generate incidents.

---

**Status**: Python packaging issues fixed, application ready for testing and deployment.
**Last Updated**: Current session
**Working Directory**: `/Users/wopseeion/Programming/Hackathons/Googe_catalyst/Google_ai_catalyst_solution`</parameter>
